---
layout: post
title: "TGI Review - server"
date: 2024-08-24 09:24:00 +0900
categories: IT
author: dh.ihm
---

[ì°¸ê³  ìë£Œ: TGI Server - server.py](https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/server.py)

## TextGenerationService class

ì´ classëŠ” text generatoinê³¼ ê´€ë ¨ëœ ì—¬ëŸ¬ gRPC methodë¥¼ í¬í•¨í•©ë‹ˆë‹¤. 

ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 

```python
class TextGenerationService(generate_pb2_grpc.TextGenerationServiceServicer):
```

generate_pb2_grpc.TextGenerationServiceServicerë¥¼ ìƒì†ë°›ì•„ì„œ êµ¬í˜„í•˜ê³  ìˆìŠµë‹ˆë‹¤. 

ì´ë¦„ì—ì„œ ìœ ì¶”í•´ ë³¼ ìˆ˜ ìˆë“¯ì´, generate_pb2_grpc.TextGenerationServiceServicerëŠ” gRPC frameworkì—ì„œ ìë™ìœ¼ë¡œ ìƒì„±ëœ ì½”ë“œì…ë‹ˆë‹¤. 

gRPCì— ëŒ€í•œ ë‚´ìš©ì€ ê°„ëµí•˜ê²Œë§Œ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. :stuck_out_tongue:

gRPCëŠ” Googleì—ì„œ ê°œë°œí•œ RPC frameworkë¡œ, protobufë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. 

protobufëŠ” êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ì§ë ¬í™”í•˜ê³ , ì—­ì§ë ¬í™”í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ë°ì´í„° í¬ë§·ì…ë‹ˆë‹¤.

protobufì— ì•„ë˜ì™€ ê°™ì´ serviceë¥¼ ì •ì˜í•˜ë©´, gRPPC frameworkì—ì„œ ì´ serviceë¥¼ ê¸°ë°˜ìœ¼ë¡œ service interface ì½”ë“œë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•´ ì¤ë‹ˆë‹¤. 

```proto
service TextGenerationService {
  rpc MethodA(InputParam) returns (OutputResponse) {}
  rpc MethodB(GenerateTextsRequest) returns (GenerateTextsResponse) {}
  ...
}
``` 

ë”°ë¼ì„œ generate_pb2_grpc.TextGenerationServiceServicerì— ëŒ€í•œ service ëª…ì„¸ëŠ” protobuf íŒŒì¼ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆê² ì£ . 

ì—¬ê¸°ì—ì„œëŠ” ì¼ë‹¨ ì‚´í´ ë³´ì§€ëŠ” ì•Šê² ìŠµë‹ˆë‹¤. 

### __init__()

__init__() methodëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ì£¼ì–´ì§„ model, cache, server URLì„ ì„¤ì •í•˜ê³ , model deviceê°€ cudaì¸ ê²½ìš°ì—ëŠ” inference modeë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

```python
    def __init__(
        self,
        model: Model,
        cache: Cache,
        server_urls: List[str],
    ):
        self.cache = cache
        self.model = model
        # Quantize is resolved during model loading
        self.quantize = model.quantize
        self.server_urls = server_urls
        # For some reason, inference_mode does not work well with GLOO which we use on CPU
        if model.device.type == "cuda":
            # Force inference mode for the lifetime of TextGenerationService
            self._inference_mode_raii_guard = torch._C._InferenceMode(True)
```

Model ê°ì²´ëŠ” í…ìŠ¤íŠ¸ ìƒì„±ì„ ì²˜ë¦¬í•˜ëŠ” modelì´ê² ì£ . GPTë‚˜ BERTì™€ ê°™ì€ ëª¨ë¸ì¼ ê²ƒì…ë‹ˆë‹¤. 

Cache ê°ì²´ëŠ” ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ëŠ” í˜„ì¬ ì½”ë“œë§Œìœ¼ë¡œëŠ” ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì½”ë“œë¥¼ ì¢€ ë” ì‚´í´ ë³´ë©´ì„œ ì•Œì•„ ë³´ê² ìŠµë‹ˆë‹¤. 

server_urlsëŠ” serverì˜ URLì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ ì¢€ ë” ì½”ë“œë¥¼ ì‚´í´ë´ì•¼ ê² ë„¤ìš”. 

ë§ˆì§€ë§‰ìœ¼ë¡œ, modelì˜ device typeì´ "cuda" ì¸ ê²½ìš°, PyTorchì˜ _InferenceModeë¥¼ í™œì„±í™”í•˜ì—¬, í…ìŠ¤íŠ¸ ìƒì„± ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ë˜ëŠ” ë™ì•ˆ ëª¨ë¸ì„ ì¶”ë¡  ëª¨ë“œë¡œ ê°•ì œ ì„¤ì •í•©ë‹ˆë‹¤

**ì¶”ë¡  ëª¨ë“œ(Inference Mode)**ëŠ” ëª¨ë¸ì´ í•™ìŠµ ëª¨ë“œì™€ëŠ” ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰ë˜ë„ë¡ í•©ë‹ˆë‹¤. GPU í™˜ê²½ì—ì„œ ëª¨ë¸ì˜ ì¶”ë¡  ì„±ëŠ¥ì„ ìµœì í™” í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì£¼ì„ì„ ë³´ë©´, [GLOO: ë¶„ì‚° ì»´í“¨íŒ… í”„ë ˆì„ì›Œí¬](https://github.com/facebookincubator/gloo)ë¥¼ ì‚¬ìš©í•˜ëŠ” CPU í™˜ê²½ì—ì„œëŠ” ì´ ëª¨ë“œê°€ ì˜ ë™ì‘í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, CUDA ì¥ì¹˜ì—ì„œë§Œ ì‚¬ìš©ë˜ë„ë¡ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 

ì–´ë–¤ ì´ìœ ì¸ì§€ ê¶ê¸ˆí•˜ë„¤ìš”. ğŸ˜ 

PyTorchì˜ inference modeì— ëŒ€í•´ì„œ ì¢€ ë” ì•Œì•„ ë³´ê² ìŠµë‹ˆë‹¤. 

inference modeì—ì„œëŠ” Autogradì™€ ì—°ê´€ëœ ì—°ì‚° ê¸°ë¡ ë¹„í™œì„±, ë¶ˆí•„ìš”í•œ graph ìƒì„± ë° ë©”ëª¨ë¦¬ í• ë‹¹ ë°©ì§€ ë“±ì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì—, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ ë° ì—°ì‚° ì†ë„ í–¥ìƒì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

ì£¼ë¡œ trainingì—ì„œ í•„ìš”í•œ ì—°ì‚°ë“¤ì„ ì œì™¸í•˜ê³ , ì¶”ë¡ ì— í•„ìš”í•œ ì—°ì‚°ë§Œ ìˆ˜í–‰í•˜ë„ë¡ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 

### Health()

ì´ methodë¥¼ ì‚¬ìš©í•´ì„œ, ì„œë²„ì˜ ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ë˜ì–´ ìˆë„¤ìš”. 

```python
async def Health(self, request, context):
        if self.model.device.type == "cuda":
            torch.zeros((2, 2)).cuda()
        return generate_pb2.HealthResponse()
```

ê°„ë‹¨í•œ ë°©ì‹ìœ¼ë¡œ, torch.zeros()ë¥¼ í˜¸ì¶œí•˜ì—¬, CUDA ì¥ì¹˜ê°€ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

ê·¸ í›„ generate_pb2.HealthResponse()ë¥¼ ë°˜í™˜í•˜ì—¬, ì„œë²„ì˜ ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. 

```python
    return generate_pb2.HealthResponse()
```

ì´ methodëŠ” ë§¤ìš° ê°„ë‹¨í•˜ê²Œ êµ¬í˜„ë˜ì–´ ìˆê³ , cuda deviceì˜ ê²½ìš°ì—ëŠ” ì§ì ‘ì ìœ¼ë¡œ tensorë¥¼ ìƒì„±í•˜ì—¬ ë™ì‘ì„ í™•ì¸í•˜ê¸° ë•Œë¬¸ì—, 

ë¹ˆë²ˆí•˜ê²Œ í˜¸ì¶œë˜ì§€ ì•ŠëŠ”ë‹¤ë©´, ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šì„ ê²ƒì…ë‹ˆë‹¤. 

ë‹¨ìˆœ ë™ì‘ í™•ì¸ ì™¸ ë‹¤ë¥¸ ì •ë³´ë¥¼ í™•ì¸í•˜ê³  ì‹¶ì„ ìˆ˜ ìˆì„ ê²ƒ ê°™ì€ë°, ì´ ë¶€ë¶„ì€ ì–´ë–»ê²Œ ì§€ì›í•˜ê³  ìˆëŠ”ì§€ ì¢€ ë” ì‚´í´ ë³´ì•„ì•¼ í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.


### Warmup()

ì´ methodëŠ” ëª¨ë¸ì˜ ìµœì í™”ë¥¼ ìœ„í•´, ì‚¬ìš© ê°€ëŠ¥í•œ ìµœëŒ€ í† í° ìˆ˜ë¥¼ ì‚¬ì „ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

ì–‘ìí™” ë°©ì‹ì´ "exl2"ë‚˜ "gptq"ì¸ ê²½ìš°ì—ëŠ”, ì¶”ê°€ ë™ì‘ì„ ë¨¼ì € í•˜ê³  ìˆìŠµë‹ˆë‹¤. 

GPTQ(ì €ë¹„íŠ¸ ì–‘ìí™”)ì™€ ê°™ì€ íŠ¹ì • ì–‘ìí™” ë°©ì‹ì€ ê³ ìœ ì˜ ì—°ì‚° ì»¤ë„ì„ í•„ìš”ë¡œ í•˜ë©°, ì´ ì»¤ë„ë“¤ì€ model ë¡œë“œ í›„ì— ìµœì¢… í˜•íƒœê°€ ê²°ì •ë©ë‹ˆë‹¤. 

ì´ë¥¼ ìœ„í•´ `create_exllama_buffers`ë¥¼ í˜¸ì¶œí•˜ì—¬ í•„ìš”í•œ ë²„í¼ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.

create_exllama_buffers() í•¨ìˆ˜ëŠ” GPTQì™€ ê°™ì€ ì–‘ìí™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•  ë•Œ í•„ìš”í•œ ë²„í¼ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. 

ì´ í•¨ìˆ˜ëŠ” íŠ¹íˆ ExLlama ì»¤ë„(ExLlama kernels)ê³¼ ê´€ë ¨ëœ ë©”ëª¨ë¦¬ ë²„í¼ë¥¼ ì„¤ì •í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. 

ExLlamaëŠ” GPT ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì–‘ìí™”í•˜ê³ , ì´ë¥¼ ë¹ ë¥´ê²Œ ì¶”ë¡ í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ìµœì í™”ëœ ì»¤ë„ì…ë‹ˆë‹¤.

ë²„í¼ë¥¼ í• ë‹¹í•˜ê¸° ìœ„í•œ íŒŒë¼ë¯¸í„°ë¡œ `max_prefill_tokens`ì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. 

ì‚¬ì „ì— ì±„ìš¸ ìˆ˜ ìˆëŠ” ìµœëŒ€ í† í° ìˆ˜ì— ë§ì¶° ë©”ëª¨ë¦¬ ë²„í¼ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ì£ .

ë²„í¼ë¥¼ í• ë‹¹í•˜ê¸° ì „ì— `set_device`ë¥¼ í˜¸ì¶œí•˜ì—¬, ëª¨ë¸ì´ ì‚¬ìš©í•˜ëŠ” deviceë¥¼ ì„¤ì •í•˜ê³ , í•´ë‹¹ deviceì— ë§ê²Œ ë²„í¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```python
async def Warmup(self, request, context):
    if self.quantize in {"exl2", "gptq"}:
        try:
            # When using GPTQ, Exllama kernels need some global kernels
            # For which we have the finale shapes only after the model has loaded
            # This will allocate those buffers.
            from text_generation_server.layers.gptq import (
                create_exllama_buffers,
                set_device,
            )

            set_device(self.model.device)
            create_exllama_buffers(request.max_prefill_tokens)
        except ImportError:
            pass
```

ë‹¤ìŒìœ¼ë¡œëŠ” modelì˜ batch ì´ˆê¸°í™”ì™€ ê´€ë ¨ëœ ì¤‘ìš”í•œ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ìš”ì²­ëœ ë°ì´í„°(request.batch)ë¥¼ modelì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•œ í›„, í•´ë‹¹ batchë¥¼ ì‚¬ìš©í•˜ì—¬ modelì˜ warmup ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. 

ì´ ì¡°ê±´ë¬¸ì„ í†µí•´ í˜„ì¬ ëª¨ë¸ì˜ batch_typeì´ VLM_BATCH_TYPESì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

VLM_BATCH_TYPESëŠ” íŠ¹ì • ëª¨ë¸ ë°°ì¹˜ ìœ í˜•ì„ ë‚˜íƒ€ë‚´ëŠ” ì§‘í•©(set)ìœ¼ë¡œ, ì´ ëª¨ë¸ë“¤ì´ íŠ¹ë³„í•œ ì´ˆê¸°í™” ê³¼ì •ì„ í•„ìš”ë¡œ í•œë‹¤ëŠ” ê²ƒ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
    if self.model.batch_type in VLM_BATCH_TYPES:
```

VLM_BATCH_TYPESì— í¬í•¨ë˜ì–´ ìˆëŠ” ê²½ìš°, modelì˜ batch ì´ˆê¸°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

```python
    batch = self.model.batch_type.from_pb_processor(
        request.batch,
        self.model.tokenizer,
        self.model.processor,
        self.model.model.config,
        self.model.dtype,
        self.model.device,
    )
```

from_pb_processorë¼ëŠ” ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ë¥¼ ì´ˆê¸°í™” í•˜ê³  ìˆëŠ”ë°, from_pb() ë³´ë‹¤ self.model.processorì™€ self.model.model.configë¥¼ ì¶”ê°€ë¡œ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ì´ methodëŠ” gRPC requestë¡œë¶€í„° ë°›ì€ ë°ì´í„°(protobuf)ë¥¼ modelì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

ì•„ë˜ ì½”ë“œë¥¼ ì°¸ê³ í•´ë³´ë©´, processorì™€ configê°€ ì‚¬ìš©ë˜ëŠ” ë¶€ë¶„ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

[ì°¸ê³  ìë£Œ: vlm_causal_lm.py](https://github.com/huggingface/text-generation-inference/blob/f3c5d7d92f005c3cd6a801a33334fb9ba32f55f8/server/text_generation_server/models/vlm_causal_lm.py)

```python
    @classmethod
    def from_pb_processor(
        cls,
        pb: generate_pb2.Batch,
        tokenizer: PreTrainedTokenizerBase,
        processor,
        config,
        dtype: torch.dtype,
        device: torch.device,
    ) -> "VlmCausalLMBatch":
        batch_tokenized_inputs, image_inputs = cls.batch_tokenized_inputs(
            pb.requests, tokenizer, processor, config
        )
        batch = cls.from_tokenized(pb, tokenizer, batch_tokenized_inputs, dtype, device)
        if image_inputs is not None:
            batch.pixel_values = image_inputs["pixel_values"].to(device=device)
            if "pixel_attention_mask" in image_inputs:
                batch.pixel_attention_mask = image_inputs["pixel_attention_mask"].to(
                    device=device
                )
            else:
                batch.pixel_attention_mask = None
            if "image_sizes" in image_inputs:
                batch.image_sizes = image_inputs["image_sizes"].to(device=device)
            else:
                batch.image_sizes = None
        else:
            batch.pixel_values = None
            batch.pixel_attention_mask = None
            batch.image_sizes = None
        return batch
```

batch typeì´ VLM_BTACH_TYPESì— í¬í•¨ë˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ëŠ”, ì•„ë˜ì™€ ê°™ì´ batchë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

```python
    else:
        batch = self.model.batch_type.from_pb(
            request.batch, self.model.tokenizer, self.model.dtype, self.model.device
        )
```

ê·¸ í›„ ì´ˆê¸°í™”ëœ batchë¥¼ ì‚¬ìš©í•˜ì—¬ modelì˜ warmup ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. 

ì´ warmup ê³¼ì •ì€ modelì´ ì´í›„ì— ë“¤ì–´ì˜¬ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•„ìš”í•œ ì¤€ë¹„ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

warmup ê³¼ì •ì—ì„œ ê³„ì‚°ëœ ìµœëŒ€ ì§€ì› í† í° ìˆ˜ê°€ max_supported_total_tokens ë³€ìˆ˜ì— ì €ì¥ë©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì…ë ¥ì˜ í¬ê¸°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

```python
    max_supported_total_tokens = self.model.warmup(batch)
```

ë§ˆì§€ë§‰ìœ¼ë¡œ, generate_pb2.WarmupResponse ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.

gRPC ì„œë²„ì—ì„œ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì‘ë‹µì„ ë³´ë‚´ëŠ” ë¶€ë¶„ìœ¼ë¡œ, WarmupResponseë¼ëŠ” íŠ¹ì • ì‘ë‹µ ë©”ì‹œì§€ë¥¼ ìƒì„±í•˜ê³ ,

ê·¸ ì•ˆì— ì´ì „ì— ê³„ì‚°ëœ max_supported_total_tokens ê°’ì„ í¬í•¨ì‹œí‚µë‹ˆë‹¤.

í´ë¼ì´ì–¸íŠ¸ëŠ” ì´ responseë¥¼ ë°›ì•„ modelì´ ì›Œë°ì—…ì„ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œí–ˆëŠ”ì§€, ê·¸ë¦¬ê³  ì–¼ë§ˆ ì •ë„ì˜ ì…ë ¥ í¬ê¸°ê¹Œì§€ ì§€ì›í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
    return generate_pb2.WarmupResponse(
            max_supported_total_tokens=max_supported_total_tokens
        )
```

### Prefill()

ì´ Prefill methodëŠ” modelì— ì…ë ¥ ë°ì´í„°ë¥¼ ì£¼ì…í•˜ê³  í† í°ì„ ìƒì„±í•˜ëŠ” ê³¼ì •ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```python
if self.model.batch_type in VLM_BATCH_TYPES:
    batch = self.model.batch_type.from_pb_processor(
        request.batch,
        self.model.tokenizer,
        self.model.processor,
        self.model.model.config,
        self.model.dtype,
        self.model.device,
    )
else:
    batch = self.model.batch_type.from_pb(
        request.batch, self.model.tokenizer, self.model.dtype, self.model.device
    )
```

warmup() methodì™€ ìœ ì‚¬í•˜ê²Œ, VLM_BATCH_TYPESì— í¬í•¨ë˜ì–´ ìˆëŠ” ê²½ìš°ì—ëŠ” from_pb_processor()ë¥¼ ì‚¬ìš©í•˜ì—¬ batchë¥¼ ì´ˆê¸°í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ê·¸ ì´í›„, generate_token methodë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ˆê¸°í™”ëœ batchë¡œ modelì—ì„œ í† í°ì„ ìƒì„±í•©ë‹ˆë‹¤. 

ì´ methodëŠ” `generations`, `next_batch`, `timings`ë¥¼ ë°˜í™˜í•˜ê³  ìˆìŠµë‹ˆë‹¤. 

`generations`ëŠ” ìƒì„±ëœ í† í°ì„ ë‚˜íƒ€ë‚´ë©°, `next_batch`ëŠ” ë‹¤ìŒ ì²˜ë¦¬ì— ì‚¬ìš©í•  batchë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

`timings`ëŠ” ê° ë‹¨ê³„ì˜ ì‹¤í–‰ ì‹œê°„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

```python
    generations, next_batch, timings = self.model.generate_token(batch)
```

ë‹¤ìŒ ìš”ì²­ì—ì„œ ì‚¬ìš©í•  next_batchë¥¼ cacheì— ì €ì¥í•©ë‹ˆë‹¤. ì´ëŠ” modelì´ ìƒíƒœë¥¼ ìœ ì§€í•˜ë©´ì„œ ì—°ì†ì ì¸ ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

```python
    self.cache.set(next_batch)
```

ë§ˆì§€ë§‰ìœ¼ë¡œ, PrefillResponseë¥¼ ìƒì„±í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤. 

generation.to_pb()ë¥¼ ì‚¬ìš©í•˜ì—¬, ìƒì„±ëœ í† í°ì„ protobuf í˜•íƒœë¡œ ë³€í™˜í•˜ê³ , next_batch.to_pb()ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ batchë¥¼ protobuf í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

ê·¸ë¦¬ê³  ê° ë‹¨ê³„ì˜ ì‹¤í–‰ ì‹œê°„ì„ timingsì— ì €ì¥í•˜ê³ , ì „ì²´ ì‹¤í–‰ ì‹œê°„ì„ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.

```python
    return generate_pb2.PrefillResponse(
        generations=[generation.to_pb() for generation in generations],
        batch=next_batch.to_pb() if next_batch else None,
        forward_ns=timings[0],
        decode_ns=timings[1],
        total_ns=time.time_ns() - start,
    )
```

prefillì€ text generation ê³¼ì •ì—ì„œ modelì˜ ì…ë ¥ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³ , ì´ˆê¸° ê²°ê³¼ë¥¼ ìƒì„±í•˜ëŠ” ì²« ë‹¨ê³„ë¡œ, ì´í›„ ì‘ì—…ì˜ ì„±ëŠ¥ê³¼ í’ˆì§ˆì— ì˜í–¥ì„ ë¼ì¹˜ê²Œ ë˜ê¸° ë•Œë¬¸ì—, 

ë§¤ìš° ì¤‘ìš”í•œ ë‹¨ê³„ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ sequence data (text)ë¥¼ ì²˜ë¦¬í•˜ëŠ”ë°, ì´ ë°ì´í„°ëŠ” ëª¨ë¸ì— ì£¼ì…ë˜ê¸° ì „ì— íŠ¹ì •í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

ë°”ë¡œ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”(tokenization) í•˜ê³ , í•„ìš”í•œ ê²½ìš° padding, truncation, special token ì¶”ê°€ ë“±ì˜ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ì£ .

prefill ê³¼ì •ì—ì„œëŠ” í´ë¼ì´ì–¸íŠ¸ê°€ ìš”ì²­í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë°›ì•„ ì´ë¥¼ tokenizerë¥¼ í†µí•´ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ê³ , í•„ìš”ì— ë”°ë¼ ì „ì²˜ë¦¬ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ì •ê·œí™”í•˜ê±°ë‚˜ ëª¨ë¸ì˜ ì„¤ì •ì— ë§ê²Œ ì¡°ì •í•©ë‹ˆë‹¤.

### Decode()

Decode()ëŠ” ì´ì „ì— ì²˜ë¦¬ëœ ë°°ì¹˜ë¥¼ ë°›ì•„ì„œ ì´ë¥¼ ë””ì½”ë”©í•˜ê³  ìƒˆë¡œìš´ í† í°ì„ ìƒì„±í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. 

ì´ methodëŠ” ì—¬ëŸ¬ batchë¥¼ ê²°í•©í•˜ì—¬ ì²˜ë¦¬í•˜ê±°ë‚˜ ë‹¨ì¼ batchë¥¼ ì²˜ë¦¬í•œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” ê³¼ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.

ë§¨ ì²˜ìŒìœ¼ë¡œ, í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ì „ë‹¬ëœ `request.batches`ê°€ ë¹„ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. 

batchê°€ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš° ì˜ˆì™¸(ValueError)ë¥¼ ë°œìƒì‹œì¼œ, ìµœì†Œí•œ í•˜ë‚˜ì˜ batchëŠ” ì œê³µë˜ì–´ì•¼ í•¨ì„ ëª…ì‹œì ìœ¼ë¡œ ì•Œë ¤ì¤ë‹ˆë‹¤.

```python
    if len(request.batches) == 0:
        raise ValueError("Must provide at least one batch")
```

ê·¸ë¦¬ê³ , `request.batches`ì— ìˆëŠ” ê° batchì˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ cacheì—ì„œ batchë¥¼ ë³µì›(pop)í•©ë‹ˆë‹¤.

```python
    batches = []
    for batch_pb in request.batches:
        batch = self.cache.pop(batch_pb.id)
        if batch is None:
            raise ValueError(f"Batch ID {batch_pb.id} not found in cache.")
        batches.append(batch)
```

ê° batchë¥¼ batches ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.

ë‹¤ìŒìœ¼ë¡œ, ì²˜ë¦¬í•  ë°ì´í„° (ë³µêµ¬ëœ batches)ê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì—†ìœ¼ë©´, ì—ëŸ¬ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.

```python
    if len(batches) == 0:
        raise ValueError("All batches are empty")
```

ì²˜ë¦¬í•  ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°, ì¦‰ ë³µì›ëœ batchê°€ í•˜ë‚˜ ì´ìƒì´ë©´, ì´ batchë“¤ì„ í•˜ë‚˜ë¡œ ê²°í•©í•©ë‹ˆë‹¤. (concatenate)

batchê°€ í•˜ë‚˜ë¿ì´ë©´ ê²°í•© ê³¼ì •ì´ í•„ìš” ì—†ìœ¼ë¯€ë¡œ, ì²« ë²ˆì§¸ batchë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
    if len(batches) > 1:
        start_concat = time.time_ns()
        batch = self.model.batch_type.concatenate(batches)
        concat_ns = time.time_ns() - start_concat
    else:
        batch = batches[0]
        concat_ns = None
```

ê·¸ë¦¬ê³  ê²°í•©ëœ(ë˜ëŠ” ë‹¨ì¼) batchë¥¼ ì‚¬ìš©í•˜ì—¬ modelì—ì„œ í† í°ì„ ìƒì„±í•©ë‹ˆë‹¤. generate_token ë©”ì„œë“œì˜ ë°˜í™˜ ê°’ì€ ìœ„ì—ì„œ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤. 

```python
    generations, next_batch, timings = self.model.generate_token(batch)
```

ì´í›„, ë‹¤ìŒ ìš”ì²­ì—ì„œ ì‚¬ìš©í•  next_batchë¥¼ cacheì— ì €ì¥í•©ë‹ˆë‹¤.

```python
    self.cache.set(next_batch)
```

ë§ˆì§€ë§‰ìœ¼ë¡œ, DecodeResponseë¥¼ ìƒì„±í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.

```python
    return generate_pb2.DecodeResponse(
        generations=[generation.to_pb() for generation in generations],
        batch=next_batch.to_pb() if next_batch else None,
        concat_ns=concat_ns,
        forward_ns=timings[0],
        decode_ns=timings[1],
        total_ns=time.time_ns() - start,
    )
```

### serve()

ì´ ì½”ë“œì—ì„œëŠ” ë‚´ë¶€ì—ì„œ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰ë˜ëŠ” serve_inner í•¨ìˆ˜ë¥¼ í†µí•´ í…ìŠ¤íŠ¸ ìƒì„± modelì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ”

gRPC ì„œë²„ë¥¼ ì„¤ì •í•˜ê³  ì‹¤í–‰í•˜ëŠ” ì „ì²´ì ì¸ íë¦„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ ì„œë²„ëŠ” ìš°ë¦¬ê°€ ì˜ˆìƒí•˜ëŠ” ê²ƒì²˜ëŸ¼ modelì„ ì´ˆê¸°í™”í•˜ê³ , í´ë¼ì´ì–¸íŠ¸ì˜ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

```python
    def serve(
        model_id: str,
        lora_adapters: Optional[List[AdapterInfo]],
        revision: Optional[str],
        sharded: bool,
        quantize: Optional[str],
        speculate: Optional[int],
        dtype: Optional[str],
        trust_remote_code: bool,
        uds_path: Path,
        max_input_tokens: int,
    ):
```

ë¨¼ì € serve() methodì˜ paraemterë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

model_id: ëª¨ë¸ì˜ ê³ ìœ  ì‹ë³„ì. ë¡œë“œí•  ëª¨ë¸ì„ ì§€ì •í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. 

lora_adapters: LoRA(LoRAëŠ” ëª¨ë¸ íŒŒì¸íŠœë‹ì„ ìœ„í•œ ì–´ëŒ‘í„°) ì–´ëŒ‘í„°ì˜ ì •ë³´ ëª©ë¡. íŠ¹ì • ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¡°ì •í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. 

sharded: ëª¨ë¸ì´ ì—¬ëŸ¬ ë…¸ë“œì— ë¶„ì‚°ë˜ì–´ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¶€ìš¸ ê°’. ë¶„ì‚° í•™ìŠµì„ ìœ„í•œ ì„¤ì •ì…ë‹ˆë‹¤. 

quantize: ëª¨ë¸ì„ ì–‘ìí™”í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•. ì„±ëŠ¥ ìµœì í™” ë˜ëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì ˆê°ì„ ìœ„í•´ ëª¨ë¸ì„ ì–‘ìí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

speculate: ì‚¬ì „ ì¶”ì¸¡ ì‘ì—…ì„ ìœ„í•œ ì„ íƒì  ì •ìˆ˜ ê°’. ì´ ê°’ì€ ë¯¸ë˜ì˜ ê³„ì‚°ì„ ë¯¸ë¦¬ ìˆ˜í–‰í•˜ì—¬ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

dtype: ëª¨ë¸ì´ ì‚¬ìš©í•  ë°ì´í„° íƒ€ì…(ì˜ˆ: float32, float16 ë“±).

trust_remote_code: ì™¸ë¶€ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ ì½”ë“œê°€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ê²°ì •í•˜ëŠ” ê°’. ë³´ì•ˆì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.

uds_path: Unix ë„ë©”ì¸ ì†Œì¼“ ê²½ë¡œ. ì„œë²„ê°€ ì‚¬ìš©í•˜ëŠ” ì†Œì¼“ì˜ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.

max_input_tokens: ì…ë ¥ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆëŠ” ìµœëŒ€ í† í° ìˆ˜ ì…ë‹ˆë‹¤. 

serve_inner() ì—ì„œëŠ” ìœ„ì—ì„œ ì„¤ëª…í•œ parameterë¥¼ ì‚¬ìš©í•˜ì—¬ gRPC ì„œë²„ë¥¼ ì„¤ì •í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.

ë¨¼ì € ì„œë²„ì˜ ì„¤ì • ê³¼ì •ì—ì„œ Unix ë„ë©”ì¸ ì†Œì¼“ì„ ì‚¬ìš©í•˜ëŠ” gRPC ì„œë²„ì˜ URLì„ ìƒì„±í•˜ê³  ì„¤ì •í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. 

íŠ¹íˆ, ì„œë²„ê°€ ë¶„ì‚° ì²˜ë¦¬(ìƒ¤ë”©) í™˜ê²½ì—ì„œ ì‘ë™í•˜ëŠ” ê²½ìš°, ê° ë…¸ë“œì— ê³ ìœ í•œ URLì„ í• ë‹¹í•˜ëŠ” ê³¼ì •ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```python
    unix_socket_template = "unix://{}-{}"
        adapter_to_index = {}
        if sharded:
            server_urls = [
                unix_socket_template.format(uds_path, rank)
                for rank in range(int(os.environ["WORLD_SIZE"]))
            ]
            local_url = server_urls[int(os.environ["RANK"])]
        else:
            local_url = unix_socket_template.format(uds_path, 0)
            server_urls = [local_url]

```

í™˜ê²½ ë³€ìˆ˜ WORLD_SIZEëŠ” ì „ì²´ ë…¸ë“œì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ê³ , RANKëŠ” í˜„ì¬ ë…¸ë“œì˜ ìˆœì„œ(index)ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

server_urls ë¦¬ìŠ¤íŠ¸ëŠ” ê° ë…¸ë“œì— ëŒ€í•´ ê³ ìœ í•œ ì„œë²„ URLì„ ìƒì„±í•©ë‹ˆë‹¤.

ê° ë…¸ë“œì˜ ì¸ë±ìŠ¤(rank)ì— ë”°ë¼ uds_pathì™€ rankë¥¼ ê²°í•©í•œ Unix ë„ë©”ì¸ ì†Œì¼“ URLì´ ìƒì„±ë©ë‹ˆë‹¤. 

ì˜ˆë¥¼ ë“¤ì–´, uds_pathê°€ /tmp/socketì´ê³  rankê°€ 1ì´ë¼ë©´, URLì€ unix:///tmp/socket-1ì´ ë©ë‹ˆë‹¤.

ê° ë…¸ë“œëŠ” ìì‹ ë§Œì˜ ë¡œì»¬ URLì„ ê°€ì ¸ì•¼ í•˜ë¯€ë¡œ, server_urls ë¦¬ìŠ¤íŠ¸ì—ì„œ ìì‹ ì˜ ì¸ë±ìŠ¤(rank)ì— í•´ë‹¹í•˜ëŠ” URLì„ ì„ íƒí•©ë‹ˆë‹¤.

shardedê°€ Falseì´ë©´, ì„œë²„ëŠ” ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ë™ì‘í•˜ë‹ˆê¹Œ, ë…¸ë“œ ì¸ë±ìŠ¤(rank)ëŠ” 0ìœ¼ë¡œ ì„¤ì •ë˜ë©°, ë‹¨ì¼ ì„œë²„ URLë§Œ ìƒì„±ë©ë‹ˆë‹¤.

server_urls ë¦¬ìŠ¤íŠ¸ì—ëŠ” ë‹¨ í•˜ë‚˜ì˜ URLë§Œ í¬í•¨ë©ë‹ˆë‹¤.

```python
    else:
        local_url = unix_socket_template.format(uds_path, 0)
        server_urls = [local_url]
```

ê·¸ ì´í›„, get_model_with_lora_adapters()ë¥¼ í˜¸ì¶œí•˜ì—¬ modelì„ ì´ˆê¸°í™”í•˜ê³ , ì´ ê³¼ì •ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì˜ˆì™¸ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```python
    model = get_model_with_lora_adapters(
        model_id,
        lora_adapters,
        revision,
        sharded,
        quantize,
        speculate,
        dtype,
        trust_remote_code,
        max_input_tokens,
        adapter_to_index,
    )
```

modelì´ ì´ˆê¸°í™” ë˜ë©´, gRPC serverë¥¼ ì´ˆê¸°í™” í•©ë‹ˆë‹¤. 

ë¨¼ì € set_adapter_to_index()ë¥¼ í˜¸ì¶œí•˜ì—¬, ì–´ëŒ‘í„°ì™€ ì¸ë±ìŠ¤ë¥¼ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

ì´ ë”•ì…”ë„ˆë¦¬ëŠ” ì–´ëŒ‘í„°ì™€ ì¸ë±ìŠ¤ë¥¼ ë§¤í•‘í•˜ëŠ” ì •ë³´ê°€ ì €ì¥ë˜ì–´ ìˆìœ¼ë©°, modelì˜ ì¼ë¶€ ì„¤ì •ì— ì‚¬ìš©ë©ë‹ˆë‹¤.

LoRA ì–´ëŒ‘í„°ì™€ ê°™ì€ model ì–´ëŒ‘í„°ëŠ” íŠ¹ì • ì¸ë±ìŠ¤ì— í• ë‹¹ë˜ì–´ modelì˜ íŠ¹ì • ë¶€ë¶„ì— ì ìš©ë©ë‹ˆë‹¤. 

set_adapter_to_indexëŠ” ì´ëŸ¬í•œ ì¸ë±ìŠ¤ ì„¤ì •ì„ modelì— ì ìš©í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. 

```python
    set_adapter_to_index(adapter_to_index)
        server = aio.server(
            interceptors=[
                ExceptionInterceptor(),
                UDSOpenTelemetryAioServerInterceptor(),
            ],
            options=[
                # Set the maximum possible message length: i32::MAX
                ("grpc.max_receive_message_length", (1 << 31) - 1)
            ],
        )
```

modelì´ ì—¬ëŸ¬ ê°œì˜ ì–´ëŒ‘í„°ë¥¼ ì‚¬ìš©í•  ë•Œ, ê° ì–´ëŒ‘í„°ê°€ modelì˜ ì–´ë–¤ ë¶€ë¶„ì— ì—°ê²°ë˜ì–´ì•¼ í•˜ëŠ”ì§€ë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.

ì´ set_adapter_to_index()ëŠ” ê·¸ ì„¤ì •ì„ ê´€ë¦¬í•˜ë©°, ì´í›„ ëª¨ë¸ì´ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. 

ë‹¤ìŒìœ¼ë¡œ, gRPC ì„œë²„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. aio.serverëŠ” gRPC ì„œë²„ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

serverë¥¼ ìƒì„± í•  ë•Œ interceptorsë¥¼ ì„¤ì •í•˜ê³  ìˆìŠµë‹ˆë‹¤. 

interceptorëŠ” gRPC í˜¸ì¶œ ì „ì— ì‹¤í–‰ë˜ëŠ” ì½”ë“œë¥¼ ì¶”ê°€í•˜ì—¬ ìš”ì²­/ì‘ë‹µì„ ìˆ˜ì •í•˜ê±°ë‚˜, ë¡œê¹…, ëª¨ë‹ˆí„°ë§, ì˜ˆì™¸ ì²˜ë¦¬ ë“±ì˜ ì¶”ê°€ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

ì´ë¦„ìœ¼ë¡œ ì¶”ì¸¡í•´ë³´ë©´, exceptionì´ ë°œìƒ í–ˆì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ExceptionInterceptorì™€ OpenTelemetryë¥¼ ì‚¬ìš©í•˜ì—¬ ì„œë²„ì—ì„œ ë°œìƒí•˜ëŠ” íŠ¸ëœì­ì…˜ì„ ì¶”ì í•˜ê³ 

ëª¨ë‹ˆí„°ë§ í•˜ëŠ” ìš©ë„ë¡œ ì‚¬ìš© ë  ê²ƒ ê°™ì€ UDSOpenTelemetryAioServerInterceptorë¥¼ ì„¤ì •í•˜ê³  ìˆìŠµë‹ˆë‹¤.

optionìœ¼ë¡œëŠ” gRPCê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ë©”ì‹œì§€ í¬ê¸°ë¥¼ 2GBë¡œ ì„¤ì •í•˜ê³  ìˆë„¤ìš”.

ê·¸ë¦¬ê³  generate_pb2_grpcë¥¼ ì‚¬ìš©í•˜ì—¬ `TextGenerationService`ë¼ëŠ” gRPC ì„œë¹„ìŠ¤ êµ¬í˜„ì„ ì„œë²„ì— ë“±ë¡í•©ë‹ˆë‹¤. 

```python
    generate_pb2_grpc.add_TextGenerationServiceServicer_to_server(
        TextGenerationService(model, Cache(), server_urls), server
    )
```

ê·¸ í›„, ì„œë¹„ìŠ¤ reflectionì„ í™œì„±í™”í•˜ê³ , ì„œë²„ë¥¼ íŠ¹ì • ì£¼ì†Œ(í¬íŠ¸)ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

reflectionì€ í´ë¼ì´ì–¸íŠ¸ê°€ ì„œë²„ì˜ ì„œë¹„ìŠ¤ì™€ ë©”ì„œë“œì— ëŒ€í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ë™ì ìœ¼ë¡œ ì¡°íšŒí•  ìˆ˜ ìˆê²Œ í•´ì£¼ë©°, ì„œë²„ëŠ” ì§€ì •ëœ ë¡œì»¬ URLì—ì„œ í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ì„ ë°›ì•„ë“¤ì¼ ìˆê²Œ ë©ë‹ˆë‹¤. 

ì´ë¥¼ í†µí•´ í´ë¼ì´ì–¸íŠ¸ê°€ ì„œë²„ì˜ APIë¥¼ ë¯¸ë¦¬ ì•Œì§€ ëª»í•˜ë”ë¼ë„, ì„œë²„ì—ì„œ ì œê³µí•˜ëŠ” ê¸°ëŠ¥ì„ ìë™ìœ¼ë¡œ íƒìƒ‰í•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.

```python
    SERVICE_NAMES = (
        generate_pb2.DESCRIPTOR.services_by_name["TextGenerationService"].full_name,
        reflection.SERVICE_NAME,
    )
    reflection.enable_server_reflection(SERVICE_NAMES, server)
    server.add_insecure_port(local_url)
```

`SERVICE_NAMES`ëŠ” ì„œë²„ì—ì„œ reflectoinì„ í™œì„±í™”í•  ì„œë¹„ìŠ¤ì˜ ì´ë¦„ë“¤ì„ ë‹´ê³  ìˆëŠ” íŠœí”Œì…ë‹ˆë‹¤.

`generate_pb2` ëª¨ë“ˆì˜ `DESCRIPTOR` ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ TextGenerationService ì„œë¹„ìŠ¤ì˜ ì „ì²´ ì´ë¦„(full name)ì„ ê°€ì ¸ì˜¤ëŠ”ë°,

DESCRIPTORëŠ” gRPC ì„œë¹„ìŠ¤ì™€ ë©”ì‹œì§€ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” ê°ì²´ë¡œ, .proto íŒŒì¼ì—ì„œ ì •ì˜ëœ ì„œë¹„ìŠ¤ì™€ ë©”ì‹œì§€ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.

`add_insecure_port()`ë¥¼ ì‚¬ìš©í•˜ì—¬, TLS/SSLì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì„œë²„ë¥¼ ì‹¤í–‰í•  í¬íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

ì‹¤ì œ í”„ë¡œë•ì…˜ì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì—, ë³´ì•ˆì„ ê³ ë ¤í•˜ì§€ ì•Šê³  ê°„ë‹¨í•˜ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡, ì¸ì¦ì„œë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì¸ì¦ ì—†ëŠ”(insecure) í¬íŠ¸ë¥¼ ì‚¬ìš©í•˜ê³  ìˆëŠ” ê±°ê² ì£ .

ë‹¤ìŒì€ ì„œë²„ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‹œì‘í•˜ê³ , ì„œë²„ê°€ ì‹¤í–‰ë˜ëŠ” ë™ì•ˆ ì§€ì†ì ìœ¼ë¡œ ìƒíƒœë¥¼ ëª¨ë‹ˆí„°ë§í•˜ì—¬ ì„œë²„ê°€ ì¤‘ë‹¨ë  ë•Œê¹Œì§€ ì‹¤í–‰ì„ ìœ ì§€í•˜ëŠ” ì½”ë“œ ì…ë‹ˆë‹¤. 

```python
    await server.start()

    logger.info("Server started at {}".format(local_url))
    signal_handler = SignalHandler()
    while signal_handler.KEEP_PROCESSING:
        await asyncio.sleep(0.5)
```

ì´ í•¨ìˆ˜ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜ì´ê¸° ë•Œë¬¸ì— await í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í˜¸ì¶œë˜ë©°, ì´ë¡œ ì¸í•´ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì°¨ë‹¨ë˜ì§€ ì•Šê³  ë‹¤ë¥¸ ë¹„ë™ê¸° ì‘ì—…ì„ ê³„ì† ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

ê·¸ë¦¬ê³  `SignalHandler` ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ ì„œë²„ê°€ ì¢…ë£Œë  ë•Œì˜ ì‹ í˜¸ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

`SignalHandler` ë‚´ë¶€ì— `KEEP_PROCESSING`ì´ë¼ëŠ” í”Œë˜ê·¸ê°€ í¬í•¨ë˜ì–´ ìˆì–´, ì´ í”Œë˜ê·¸ê°€ Falseë¡œ ë³€ê²½ë˜ë©´ ì„œë²„ê°€ ì¢…ë£Œë©ë‹ˆë‹¤

ì´ì œ ë‹¤ì‹œ `serve()` í•¨ìˆ˜ë¡œ ëŒì•„ì™€ì„œ, ë§ˆì§€ë§‰ êµ¬í˜„ì„ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤. 

```python
     asyncio.run(
        serve_inner(
            model_id,
            lora_adapters,
            revision,
            sharded,
            quantize,
            speculate,
            dtype,
            trust_remote_code,
        )
    )
```

 `asyncio.run()`ì„ ì‚¬ìš©í•˜ì—¬ `serve_inner` ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. 
 
 `asyncio.run()`ì€ Pythonì˜ ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì„ ì²˜ë¦¬í•˜ëŠ” ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ ì‹œì‘í•˜ê³ , ì£¼ì–´ì§„ ë¹„ë™ê¸° ì½”ë£¨í‹´ì„ ì‹¤í–‰í•˜ì—¬ ìµœì¢… ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. 

 ë‹¤ìŒ ë¦¬ë·°ì—ì„œëŠ” ì´ ì„œë²„ì™€ í•¨ê»˜ ë™ì‘í•˜ëŠ” í´ë¼ì´ì–¸íŠ¸ë¥¼ ë¦¬ë·°í•´ ë³´ê² ìŠµë‹ˆë‹¤. ğŸ˜‹
