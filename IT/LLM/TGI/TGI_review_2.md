---
layout: post
title: "TGI Review - server"
date: 2024-08-24 09:24:00 +0900
categories: IT
author: dh.ihm
---

[ì°¸ê³  ìë£Œ: TGI Server - server.py](https://github.com/huggingface/text-generation-inference/blob/main/server/text_generation_server/server.py)

## TextGenerationService class

ì´ classëŠ” text generatoinê³¼ ê´€ë ¨ëœ ì—¬ëŸ¬ gRPC methodë¥¼ í¬í•¨í•©ë‹ˆë‹¤. 

ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 

```python
class TextGenerationService(generate_pb2_grpc.TextGenerationServiceServicer):
```

generate_pb2_grpc.TextGenerationServiceServicerë¥¼ ìƒì†ë°›ì•„ì„œ êµ¬í˜„í•˜ê³  ìˆìŠµë‹ˆë‹¤. 

ì´ë¦„ì—ì„œ ìœ ì¶”í•´ ë³¼ ìˆ˜ ìˆë“¯ì´, generate_pb2_grpc.TextGenerationServiceServicerëŠ” gRPC frameworkì—ì„œ ìë™ìœ¼ë¡œ ìƒì„±ëœ ì½”ë“œì…ë‹ˆë‹¤. 

gRPCì— ëŒ€í•œ ë‚´ìš©ì€ ê°„ëµí•˜ê²Œë§Œ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. :stuck_out_tongue:

gRPCëŠ” Googleì—ì„œ ê°œë°œí•œ RPC frameworkë¡œ, protobufë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. 

protobufëŠ” êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ì§ë ¬í™”í•˜ê³ , ì—­ì§ë ¬í™”í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ë°ì´í„° í¬ë§·ì…ë‹ˆë‹¤.

protobufì— ì•„ë˜ì™€ ê°™ì´ serviceë¥¼ ì •ì˜í•˜ë©´, gRPPC frameworkì—ì„œ ì´ serviceë¥¼ ê¸°ë°˜ìœ¼ë¡œ service interface ì½”ë“œë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•´ ì¤ë‹ˆë‹¤. 

```proto
service TextGenerationService {
  rpc MethodA(InputParam) returns (OutputResponse) {}
  rpc MethodB(GenerateTextsRequest) returns (GenerateTextsResponse) {}
  ...
}
``` 

ë”°ë¼ì„œ generate_pb2_grpc.TextGenerationServiceServicerì— ëŒ€í•œ service ëª…ì„¸ëŠ” protobuf íŒŒì¼ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆê² ì£ . 

ì—¬ê¸°ì—ì„œëŠ” ì¼ë‹¨ ì‚´í´ ë³´ì§€ëŠ” ì•Šê² ìŠµë‹ˆë‹¤. 

### __init__()

__init__() methodëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ì£¼ì–´ì§„ model, cache, server URLì„ ì„¤ì •í•˜ê³ , model deviceê°€ cudaì¸ ê²½ìš°ì—ëŠ” inference modeë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

```python
    def __init__(
        self,
        model: Model,
        cache: Cache,
        server_urls: List[str],
    ):
        self.cache = cache
        self.model = model
        # Quantize is resolved during model loading
        self.quantize = model.quantize
        self.server_urls = server_urls
        # For some reason, inference_mode does not work well with GLOO which we use on CPU
        if model.device.type == "cuda":
            # Force inference mode for the lifetime of TextGenerationService
            self._inference_mode_raii_guard = torch._C._InferenceMode(True)
```

Model ê°ì²´ëŠ” í…ìŠ¤íŠ¸ ìƒì„±ì„ ì²˜ë¦¬í•˜ëŠ” modelì´ê² ì£ . GPTë‚˜ BERTì™€ ê°™ì€ ëª¨ë¸ì¼ ê²ƒì…ë‹ˆë‹¤. 

Cache ê°ì²´ëŠ” ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ëŠ” í˜„ì¬ ì½”ë“œë§Œìœ¼ë¡œëŠ” ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì½”ë“œë¥¼ ì¢€ ë” ì‚´í´ ë³´ë©´ì„œ ì•Œì•„ ë³´ê² ìŠµë‹ˆë‹¤. 

server_urlsëŠ” serverì˜ URLì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ ì¢€ ë” ì½”ë“œë¥¼ ì‚´í´ë´ì•¼ ê² ë„¤ìš”. 

ë§ˆì§€ë§‰ìœ¼ë¡œ, modelì˜ device typeì´ "cuda" ì¸ ê²½ìš°, PyTorchì˜ _InferenceModeë¥¼ í™œì„±í™”í•˜ì—¬, í…ìŠ¤íŠ¸ ìƒì„± ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ë˜ëŠ” ë™ì•ˆ ëª¨ë¸ì„ ì¶”ë¡  ëª¨ë“œë¡œ ê°•ì œ ì„¤ì •í•©ë‹ˆë‹¤

**ì¶”ë¡  ëª¨ë“œ(Inference Mode)**ëŠ” ëª¨ë¸ì´ í•™ìŠµ ëª¨ë“œì™€ëŠ” ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰ë˜ë„ë¡ í•©ë‹ˆë‹¤. GPU í™˜ê²½ì—ì„œ ëª¨ë¸ì˜ ì¶”ë¡  ì„±ëŠ¥ì„ ìµœì í™” í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì£¼ì„ì„ ë³´ë©´, GLOO(ë¶„ì‚° ì»´í“¨íŒ… í”„ë ˆì„ì›Œí¬)ë¥¼ ì‚¬ìš©í•˜ëŠ” CPU í™˜ê²½ì—ì„œëŠ” ì´ ëª¨ë“œê°€ ì˜ ë™ì‘í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, CUDA ì¥ì¹˜ì—ì„œë§Œ ì‚¬ìš©ë˜ë„ë¡ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 

ì–´ë–¤ ì´ìœ ì¸ì§€ ê¶ê¸ˆí•˜ë„¤ìš”. ğŸ˜



